\documentclass[10pt,xcolor=dvipsnames]{beamer}
\setbeamertemplate{navigation symbols}{}

\usepackage{color}
\usepackage{CREAL_slides}
\usepackage[latin1]{inputenc}
\usepackage{calc}
\usepackage[loadonly]{enumitem}
\usepackage{float}
\usepackage[position=top,singlelinecheck=off]{subfig}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{arrows}

\DeclareMathOperator{\argmax}{arg\,max}

\newcommand{\X}{\mathbf{X}}



\title[Multivariate methods in health studies]{Unsupervised Methods}
\author[Juan R Gonzalez]{Juan R Gonzalez}
\institute[ISGlobal]{BRGE - Bioinformatics Research Group in Epidemiology \\
		  Barcelona Institute for Global Health (ISGlobal) \\
		           {\tt e-mail:juanr.gonzalez@isglobal.org} \\
                  \url{http://brge.isglobal.org} \\
                  and Departament of Mathematics, UAB
                  }

\date{}


\begin{document}


<<setup, echo=FALSE>>=
options(width = 80)
knitr::opts_chunk$set(tidy=FALSE, size='footnotesize', warning=FALSE, 
               message=FALSE, fig.align='center', out.width='2in')
@



\frame{\titlepage}




\begin{frame}{Outline}
\tableofcontents
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multidimensional reduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Principal component analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[plain]{Principal component analysis}

\begin{itemize}
 \item The goal is to find a small number of independent linear combinations (principal components) of a set 
of measured variables that capture as much 
of the variability in the original variables as 
possible. 
 \item  Supplementary variables can also be considered. They are not included in the calculation of principal components and including them does not affect the results. The supplementary variables are projected on to the loading plot and used to enhance interpretation.
 \item It is a useful exploratory technique 
and can help you to create predictive models. For instance, in disease association studies, principal components are regressed against health outcomes.
\end{itemize}

\end{frame}

\begin{frame}[plain]\frametitle{Principal component analysis}

\begin{figure}
\begin{center}
\includegraphics[height=5cm, width=5cm]{figures/pca_example.jpg}
\end{center}
\end{figure}

\end{frame}



\begin{frame}[plain]\frametitle{Principal component analysis}

\begin{itemize}
\item It is used in genomic, transcriptomic or epidemiological studies to find patterns (signatures) with regard to variantes, abundance of mRNAs or exposure/diet, respectively.
\item Given a data set $X$, which is a $n \times p$ matrix, of $n$ individuals and $p$ features
$$ X=(x_1, x_2, \ldots, x_p)$$ 
  \item we look for new variables that are linear combinations of the original variables
$f=q_1X_1 + q_2X_2 + \ldots + q_pX_p$  or $f=Xq$ where $q$ are know as loadings.
\item We introduce the restriction that for $i$th component, $q$'s should maximize the variance components of $f$'s
$$ \argmax_{q^i} \text{var}(Xq^i)$$ and $q$'s has to be orthogonal to each other.
\end{itemize}

\end{frame}

\begin{frame}[fragile, plain]\frametitle{Principal component analysis}

<<plot_pca_1,  echo=FALSE>>=
set.seed(12345)
y <- rnorm(45, 4, 1)
x <- 2 * y + rnorm(45)
cc <- rep("black", 45)
cc[x>10] <- "red"
cc[c(4,9)] <- "red"
plot(x,y, cex.lab=1.4, cex.axis = 1.3, xlab="GATA3", ylab="XPBD1")
points(x,y, pch=16, col=cc, cex=1.3)
@

\end{frame}

\begin{frame}[fragile, plain]\frametitle{Principal component analysis}

<<plot_pca_2, echo=FALSE>>=
plot(x,y, cex.lab=1.4, cex.axis = 1.3, xlab="GATA3", ylab="XPBD1")
points(x,y, pch=16, col=cc, cex=1.3)
abline(lm(y~x), col="darkgreen", lwd=2)
arrows(6.5, 2.5, 6.5, 3.2, length=0.15, col="darkgreen")
text(6.5, 2.3, "First PCA", col="darkgreen")
@

\end{frame}


\begin{frame}[fragile, plain]\frametitle{Principal component analysis}

<<plot_pca_3, echo=FALSE>>=
plot(x,y, cex.lab=1.4, cex.axis = 1.3, xlab="GATA3", ylab="XPBD1")
points(x,y, pch=16, col=cc, cex=1.3)
abline(lm(y~x), col="darkgreen", lwd=2)
arrows(6.5, 2.5, 6.5, 3.2, length=0.15, col="darkgreen")
text(6.5, 2.3, "First PCA", col="darkgreen")
segments(10, 3.4, 7.6, 5.0, col="darkgreen")
arrows(11.7, 3.6, 10, 3.6, length=0.15, col="darkgreen")
text(12.0, 3.6, "Second PCA", adj=0, col="darkgreen")
@

\end{frame}




\begin{frame}[plain]{Principal component analysis}

\begin{itemize} 
 \item Correlations between variables and the principal axes are known as \textbf{loadings}
 \item Each element represents the contribution of a given variable to a component (\textbf{eigenvalues})
\end{itemize}

\end{frame}

\begin{frame}[plain]{Principal component analysis}

 \includegraphics[height=4.5cm, width=4.5cm]{figures/pca2.jpg}
 
\end{frame}


\begin{frame}[plain]{PCA: How many axes are needed?}

\begin{itemize}
 \item Does the $(k+1)^{th}$ principal axis represent more variance that would e expected by chance?
 \item Several tests and rules have been proposed (\textbf{Horn\'s method} and bootstrap approach)
 \item A common \emph{rule of thumb}, when PCA is based on correlations, is that axes with eigenvalues $> 1$ are worth interpreting
\end{itemize}
\end{frame}

\begin{frame}[plain]{PCA: non-linear relationships}

\begin{itemize}
 \item PCA assumes relationships among variables are LINEAR
 \item If the sctructure in the data is NONLINEAR (the cloud of points twists and curves its way through $p$-dimensional space) the principal axes will not be an efficient and informative summary of the data
 \item Use Principal Curve Analysis
\end{itemize}
\end{frame}

\begin{frame}[fragile, plain]{Principal components analysis with R}

<<data>>=
require(graphics)
data(USArrests)
head(USArrests)
@
\end{frame}


\begin{frame}[fragile, plain]{Principal components analysis with R}

<<pca>>=
princomp(USArrests)
mod <- princomp(USArrests)
summary(mod)
@
\end{frame}


\begin{frame}[fragile, plain]{Principal components analysis with R}

<<pca2>>=
mod2 <- princomp(USArrests)
summary(mod2)
@
\end{frame}

\begin{frame}[fragile, plain]{Principal components analysis with R}
<<plotPCA>>=
plot(mod2)
@
\end{frame}


\begin{frame}[fragile, plain]{Principal components analysis with R}
\noindent Help interpreting the results
<<pca3>>=
loadings(mod2)
@
\end{frame}


\begin{frame}[fragile, plain]{Principal components analysis with R}

<<pcaPlot3>>=
biplot(mod2)
@
\end{frame}


\begin{frame}[fragile, plain]{PCA improving visualization}

\begin{itemize}
  \item Data from the Cancer Genome Atlas (TCGA) will be analyzed.
  \item A subset of the TCGA breast cancer study from Nature 2012 publication have been selected.
  \item Data {\tt https://tcga-data.nci.nih.gov/docs/publications/brca\_2012/}.
  \item Available data are: miRNA, miRNAprecursor, RNAseq, Methylation, proteins from a RPPA array, and  GISTIC SNP calls (CNA and LOH). Clinical data are also available.
  \item We are interested in comparing women with ER+ vs ER-.
 \end{itemize}
 
\end{frame}


\begin{frame}[fragile, plain]{PCA improving visualization}

<<plotPCA2>>=
library(made4)
load("data/breast_TCGA.RData")
group <- droplevels(breast_multi$clin$ER.Status)
rnaseq <- breast_multi$RNAseq
out <- ord(rnaseq, trans=FALSE, type="pca", classvec=group)
plot(out, nlab=3, arraylabels=rep("T", 79))
@
\end{frame}


\begin{frame}[fragile, plain]{PCA improving visualization}

<<plotPCA2b>>=
par(mfrow=c(2,1))
plotarrays(out$ord$co, classvec=group)
plotgenes(out, col="blue")
@

\end{frame}

\begin{frame}[fragile, plain]{PCA improving visualization}

A list of variables with higher loadings on axes can be obtained using the following
function

<<list_genes>>=
ax1 <- topgenes(out, axis=1, n=5, ends="pos")
ax2 <- topgenes(out, axis=2, n=5, ends="neg")
cbind(pos=ax1, neg=ax2)
@

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{PCA requirements}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame}[fragile, plain]{PCA requirements}

\begin{itemize}
  \item Data scale
  <<scale>>=
    rnaseq.s <- scale(rnaseq, center= TRUE, scale = TRUE)
   @
 \item Complete cases
   <<impute>>=
   library(impute)
   rnaseq.s.imp <- impute.knn(rnaseq.s, rowmax = 0.5,
                            colmax = 0.8)$data #samples in columns!!!
   @
\end{itemize}
\end{frame}


\begin{frame}[fragile, plain]{PCA requirements}
 <<plotPCA3>>=
 out <- ord(rnaseq.s.imp, trans=FALSE, type="pca", classvec=group)
 plot(out, nlab=3, arraylabels=rep("T", 79))
@
\end{frame}

\begin{frame}[fragile, plain]{Variance explained}
 <<var_exp>>=
 summary(out$ord)
 @
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Number of significant components}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[fragile, plain]{PCA number of significant components}

\begin{itemize}
 \item  Parallel analysis (Horn) \url{http://files.eric.ed.gov/fulltext/EJ1101205.pdf}
 \item Bootstrap method (Eigenvalues)
\end{itemize}

<<boot, fig.show='hide'>>=
library(nFactors)
ev <- eigen(cor(USArrests)) # get eigenvalues
ap <- parallel(subject=nrow(USArrests), var=ncol(USArrests),
  rep=100,cent=.05)
nS <- nScree(x=ev$values, aparallel=ap$eigen$qevpea)
plotnScree(nS) 
@
\end{frame}


\begin{frame}[plain]{PCA number of significant components}

 \begin{figure}
 \begin{center}
   \includegraphics[height=5cm, width=5cm]{figure/boot-1}
 \end{center}

\end{figure}\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Principal curves}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Principal curves}

 \includegraphics{figures/pcurve.jpg}
 
\end{frame}


\begin{frame}[fragile, plain]{Principal curves}
\noindent \textbf{Example:}  Spectral decomposition of stellar objects, generated in the framework of the Gaia project.
<<lcpm>>=
require(LPCM)
data(gaia)
dim(gaia)
names(gaia)
@
\end{frame}


\begin{frame}[fragile, plain]{Principal curves}

<<lcpmPlot>>=
 lpc1 <-  lpc(gaia[,7:8])
 plot(lpc1, curvecol="red", lwd=3)
@

\end{frame}

\begin{frame}[fragile, plain]{Principal curves}

<<lcpmPlot2>>=
require(scatterplot3d)
lpc2 <- lpc(gaia[,7:9])
plot(lpc2, curvecol=2, type=c("curve","mass"))
@

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Improvements of PCA}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[fragile, plain]\frametitle{Improvements of PCA}

\begin{itemize}
\item There are other thechniques such as: Principal co-ordinate analysis (PCoA) or Multidimensional scaling (MDS), correspondence analysis (CA) and nonsymetrical correspondence analysis (NSCA).
\pause
\item They can be applied to other types of data (e.g. non-continuous). PCoA can also be applied to binary or count data.
\pause
\item PCA is designed to analyze multi-normal distributed data. If data are skweed, contain extreme outliers, or display nonlinear trends, other methods such as 
PCoA or MDS should be used instead.
\pause
\item Nonnegative matrix factorization (NMF) and Independent Component Analysis (ICA) are applied when orthogonality or independence acrross components are not hold.
\end{itemize}

\end{frame}


\begin{frame}[fragile, plain]\frametitle{Improvements of PCA}

\begin{itemize}
\item Solving the problem for the $i$-th component $$ \argmax_{q^i} \text{var}(Xq^i)$$ uses SVD decomposition and it requires and inversion step that can be problematic when   $p >> n$
\item Several extensions based on reguralization step or L-1 penalization (Least Absolute Shrinkage and Selection Operator, LASSO) can be applied
\item Sparse, penalized and reguralized extensions of PCA and related method have been recently proposed in omic data analysis.
\end{itemize}
\end{frame}


\begin{frame}[plain]\frametitle{Multidimensional scalling}

\begin{itemize}
\item PCA requires (multivariate) data normallity. This is an strong assumption.
\item Multidimensional scalling (MDS) creates a plot displaying the relative positions of a number of 
variables, given only a table of the distances between them.
\item There are two main methods for solving MDS
\begin{enumerate}
\item Classical Multidimensional Scaling reproduces the original metric or distances. 
\item Non-Metric Multidimensional Scaling assumes that only the ranks of the distances are known. Hence, this method produces a map which tries to reproduce these ranks. 
\item  MDS can be performed using {\tt cmdscale} function.
 \end{enumerate}
\end{itemize}
\end{frame}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Clustering methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[fragile, plain]{Clustering methods}

\begin{itemize}
 \item The goal is to group samples (rows) or features (columns) or both (bi-Clustering)
  according to how separated are the groups.
 \item This 'separation' is measure using a \textbf{dissimilarity measure}
 \item Classes of each individual/feature is not know
 \item Therefore, it is known as non-supervised Clustering  
\end{itemize}

\end{frame}


\begin{frame}[fragile, plain]{Clustering methods}

\begin{itemize}
 \item A good clustering method will produce hihg quality clusters with
   \begin{itemize}
    \item High intra-class similarity
    \item low inter-class similarity
   \end{itemize} 
 \item The quality of a clustering result depends on both the similarity measure used
       by the method and its implementation
 \item The quality of a clustering method is also measured by its ability to dicover hidden patterns
      
\end{itemize}

\end{frame}



\begin{frame}[fragile, plain]{Clustering methods}

\begin{itemize}
 \item \textbf{Hierarchical algorithms}: Create a hierarchical decomposition
        (agglomerative o divisive) of the set of data using some criterion
 \item \textbf{Partitioning algorithms}: Construct several partitions and the evaluate them by some
        criterion 
     \begin{itemize}
       \item k-means: each cluster is represented by the center of the cluster
       \item k-medoids (or PAM): each cluster is represented by one of the objects in the cluster
     \end{itemize}
 \item \textbf{Model-based}: A model is hypothesized for each of the clusters and the idea is to find
        the best fit of the data given the model
      
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hierarchical clustering}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile, plain]{Hierarchical clustering}
Use distance matrix as clustering criteria. This method does not require the number of clusters, but needs a termination condition.

\medskip

\includegraphics{figures/hierarchical.jpg}
\end{frame}



\begin{frame}[fragile, plain]{Hierarchical clustering}
\textbf{Distance}
\begin{itemize}
 \item Continuous variables: euclidean, manhattan, canberra, ...
 \item Categorical variables: binary
 \item Mixed variables: Gower   
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Partitioning methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile, plain]{Partitioning methods}
Construct a partition of a database into a set of k clusters

\includegraphics{figures/kmeans.jpg}
\end{frame}


\begin{frame}[fragile, plain]{Partitioning methods}
\begin{itemize}
\item Strengths
  \begin{itemize}
   \item Relatively efficient O(nk) 
   \item Often ends at a local optimum
  \end{itemize}
\item Weeknesses
 \begin{itemize}
   \item Applicable only when mean is define - what about categorical data?
   \item Need to specify $k$, the number of clusters, in advance
   \item Unable to handle noisy data and outliers
   \item Non suitable to discover clusters with non-convex shapes
  \end{itemize}

\end{itemize}


\end{frame}



\begin{frame}[fragile, plain]{Partitioning methods: k-medoids}
\begin{itemize}
\item Find representative (i.e., the most centrally located) objects, called medoids, in clusters
\item PAM (Partitioning Around Medoids)
  \begin{itemize}
    \item starts from a initial set of medoids and iteratively replaces one of the medoids by one of the non-medoids when 
        total distance is improved in the resulting clustering
    \item works fine for small data sets
    \item more robust than k-means
    \item more complex: $O(k(n-k)^2)$
 \end{itemize}
\item CLARA: Uses multiple samples
\item CLARANS: Randomized sampling
 \end{itemize}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model-based methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile, plain]{Model-based methods}
$$ f(x) = \sum_{g=1}^G \pi_G \phi(x | \mu_g, \Sigma_g) $$

where $\pi_g$ is the probability that an observation belongs to group $g$ and
$\phi(x | \mu_g, \Sigma_g)$ is the density of a multivariate Gaussian.
\begin{itemize}
 \item MCLUST
 \item Latent class approach
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Clusteing methods with R}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[fragile, plain]{Clustering methods with R}
\textbf{Example}: To illustrate interpretation of different Clustering methods, we'll look at 
a cluster analysis performed on a set of cars from 1978-1979; the data can be found at 
http://www.stat.berkeley.edu/classes/s133/data/cars.tab. Since the data is a tab-delimited file, we use read.delim:

<<>>=
dd <- read.delim("data/cars.tab")
head(dd)
@

\end{frame}

\begin{frame}[fragile, plain]{Data scalling}
It looks like the variable are measured on different scales. This requires data standardization.We will apply 'robust' normalization (scale can also be used). There are functions like
{\tt daisy} from {\tt cluster} package that will automatically perform standardization, but it doesn't give you complete control.

<<norm>>=
dd.ok <- dd[ , -c(1,2)]
medians <- apply(dd.ok, 2, median)
mads <- apply(dd.ok, 2, mad) # median absolute deviation
dd.ok <- scale(dd.ok, center=medians, scale=mads) 
@
\end{frame}


\begin{frame}[fragile, plain]{Hiherarchical methods}

<<dist>>=
dd.dist <- dist(dd.ok)
dd.dist.camb <- dist(dd.ok, method="canberra")
@

<<dist2>>=
library(cluster)
dd.dist.gower <- daisy(dd.ok, metric="gower") 
@

\end{frame}


\begin{frame}[fragile, plain]{Hiherarchical methods}
Dendogram is the main graphical tool for getting insight into a cluster solution

<<hclust>>=
dd.hclust <- hclust(dd.dist)
plot(dd.hclust, labels=dd$Car, main="")
@
\end{frame}

\begin{frame}[fragile, plain]{Hiherarchical methods}
One may be interested in knowing how many samples/features are in each group that is defined by 
a given height along the y-axis.  

<<hclustCut>>=
groups3.hclust <- as.factor(cutree(dd.hclust, 3))
table(groups3.hclust)
@
\end{frame}

\begin{frame}[fragile, plain]{Hiherarchical methods}
To see which individuals/features are in each group ...

<<hclustGroup>>=
dd$Car[groups3.hclust==1]
@


To see which individuals/features are in each group ...

<<hclustGroup2>>=
sapply(unique(groups3.hclust), function (x) dd$Car[groups3.hclust==x])
@
\end{frame}


\begin{frame}[fragile, plain]{Hiherarchical methods}
Dendrogram can be colour
<<hclustCol>>=
dend <- as.dendrogram(dd.hclust)
dend2 <- dendextend::color_labels(dend, k=3)
dendextend::labels(dend2) <- dd$Car
plot(dend2)
@
\end{frame}


\begin{frame}[fragile, plain]{Partitioning methods}
<<pam>>=
require(cluster)
dd.pam <- pam(dd.dist, 3)
dd.kmeans <- kmeans(dd.dist, 3)
groups3.pam <- as.factor(dd.pam$clustering)
groups3.kmeans <-  as.factor(dd.kmeans$cluster)
@

\end{frame}



\begin{frame}[fragile, plain]{Partitioning methods}
<<comparison>>=
table(groups3.pam, groups3.hclust)
table(groups3.pam, groups3.kmeans)
@

\end{frame}


\begin{frame}[fragile, plain]{Partitioning methods}
Silhouette plot describe how good the structure of the clusters is.
<<pamPlot>>=
plot(dd.pam)
@

\end{frame}

\begin{frame}[fragile, plain]{Partitioning methods}
There is a criteria to interpret these values

\begin{table}[h]
\begin{tabular}{ll}
\hline \hline
Range of SC & Interpretation                                \\
\hline
0.71-1.00   & A strong structure has been found             \\
0.51-0.70   & A reasonable structure has been found         \\
0.26-0.50   & The structure is weak and could be artificial \\
0-0.25      & No substantial structure has been found      \\
\hline \hline
\end{tabular}
\end{table}

\end{frame}

\begin{frame}[fragile, plain]{Partitioning methods}
Silhouette plot can be created for any method
<<pamPlotAny>>=
plot(silhouette(cutree(dd.hclust,4), dd.dist))
@

\end{frame}


\begin{frame}[fragile, plain]{Model-based}
Mclust assumes Multivariate Gaussian distribution
<<mclust>>=
require(mclust)
dd.mclust <- Mclust(dd.ok)
summary(dd.mclust)
@
\end{frame}




\begin{frame}[fragile, plain]{Model-based}
Mclust assumes Multivariate Gaussian distribution
<<mclustG>>=
groups3.mclust <- Mclust(dd.ok, G=3)$class
table(groups3.mclust, groups3.pam)
@
\end{frame}



\begin{frame}[fragile, plain]{Model-based}

<<mclustFig2D>>=
mclust2Dplot(dd.ok[,1:2], parameters=dd.mclust$parameters, 
             z=dd.mclust$z, what = "classification")
@

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Clustering related issues}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile, plain]{Clustering related issues}
Key issues
\begin{itemize}
 \item Number of clusters?
 \item Are clusters statistically significant?
 \item Are clusters reproducible? 
 \item Big datasets?
 \item Sparse data in genomics? 
\end{itemize}

\end{frame}

\begin{frame}[fragile, plain]{Number of clusters}

\begin{itemize}
 \item Model-based: AIC
 \item Clustering methods: 
 \begin{itemize}
  \item \textbf{calinski}: (SSB/(K-1))/(SSW/(n-K)), where n is the number of data points and K is the number of clusters. 
               SSW is the sum of squares within the clusters while SSB is the sum of squares among the clusters. 
               This index is simply an F (ANOVA) statistic.
  \item \textbf{ssi}: the \emph{Simple Structure Index} multiplicatively combines several elements which influence the 
                      interpretability of a partitioning solution. The best partition is indicated by the highest 
                      SSI value.
 \end{itemize} 
\end{itemize}

\end{frame}

\begin{frame}[fragile, plain]{Number of clusters}

<<selectK>>=
require(vegan)
k.cal <- cascadeKM(dd.ok, inf.gr=2, sup.gr=6, criterion="calinski")
k.ssi <- cascadeKM(dd.ok, inf.gr=2, sup.gr=6, criterion="ssi")
@

\end{frame}


\begin{frame}[fragile, plain]{Number of clusters}

<<plotCal>>=
plot(k.cal)
@

\end{frame}


\begin{frame}[fragile, plain]{Number of clusters}

<<plotSsi>>=
plot(k.ssi)
@

\end{frame}


\begin{frame}[fragile, plain]{Testing clustering results}

Test for 2 clusters vs 1 

<<plotTest>>=
require(sigclust)
mod.sig <-sigclust(dd.ok, nsim=1000)
plot(mod.sig, arg="pvalue")
@

\end{frame}


\begin{frame}[fragile, plain]{Cluster validation}
<<validation>>=
require(clValid)
val.intern <- clValid(dd.ok, 2:6, clMethods = c("hierarchical",
 "kmeans", "diana", "pam",  "model"), validation = "internal")
optimalScores(val.intern)
@
\end{frame}

\begin{frame}[fragile, plain]{Big datasets}
<<big>>=
require(fastcluster)
hclust.fast <- hclust(dd.dist)
@
\end{frame}

\end{document}





